{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZClw1TQPRdjI9Lxo1vwQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roshan-d21/Fake-News-Detector/blob/master/BERT/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvHTb06Lh0DB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.utils.data as data_utils\n",
        "import torch.optim as optim\n",
        "import gc #garbage collector for gpu memory \n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyGPhpuqGEWx"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsmIcnyTGVyd"
      },
      "source": [
        "%%capture\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "_3rk3ZxfGYK4",
        "outputId": "6693e81c-4a08-420b-b670-4f5df9acb3a0"
      },
      "source": [
        "import pandas as pd\n",
        "news_data = pd.read_csv('fake_news.csv')\n",
        "news_data.dropna(inplace=True)\n",
        "news_data['target'] = news_data.apply(lambda row: 1 if row.Label else 0, axis=1)\n",
        "news_data.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Statement</th>\n",
              "      <th>Label</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Says John McCain has done nothing to help the ...</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When asked by a reporter whether hes at the ce...</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Over the past five years the federal governmen...</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Says that Tennessee law requires that schools ...</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Says Vice President Joe Biden \"admits that the...</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Donald Trump is against marriage equality. He ...</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>We know that more than half of Hillary Clinton...</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Statement  Label  target\n",
              "0  Building a wall on the U.S.-Mexico border will...   TRUE       1\n",
              "1  Wisconsin is on pace to double the number of l...  FALSE       1\n",
              "2  Says John McCain has done nothing to help the ...  FALSE       1\n",
              "3  Suzanne Bonamici supports a plan that will cut...   TRUE       1\n",
              "4  When asked by a reporter whether hes at the ce...  FALSE       1\n",
              "5  Over the past five years the federal governmen...   TRUE       1\n",
              "6  Says that Tennessee law requires that schools ...   TRUE       1\n",
              "7  Says Vice President Joe Biden \"admits that the...  FALSE       1\n",
              "8  Donald Trump is against marriage equality. He ...   TRUE       1\n",
              "9  We know that more than half of Hillary Clinton...  FALSE       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rta8qX8HAVW"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4PBMJx1IEVQ"
      },
      "source": [
        "tokenized_df = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], news_data['Statement']))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mulTUn2uIIY4"
      },
      "source": [
        "totalpadlength = 512"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V01XQK9JINJ-"
      },
      "source": [
        "indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, tokenized_df))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL_N_WLgIzIN"
      },
      "source": [
        "index_padded = np.array([xi + [0] * (totalpadlength - len(xi)) for xi in indexed_tokens])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0U8_vIMIWMq"
      },
      "source": [
        "target_variable = news_data['target'].values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8p140adIdMG"
      },
      "source": [
        "all_words = []\n",
        "for l in tokenized_df:\n",
        "  all_words.extend(l)\n",
        "all_indices = []\n",
        "for i in indexed_tokens:\n",
        "  all_indices.extend(i)\n",
        "\n",
        "word_to_ix = dict(zip(all_words, all_indices))\n",
        "ix_to_word = dict(zip(all_indices, all_words))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxaegzsgImJS"
      },
      "source": [
        "mask_variable = [[float(i>0) for i in j] for j in index_padded]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "656-Um9uM8HR"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "def format_tensors(text_data, mask, labels, batch_size):\n",
        "    X = torch.from_numpy(text_data)\n",
        "    X = X.long()\n",
        "    mask = torch.tensor(mask)\n",
        "    y = torch.from_numpy(labels)\n",
        "    y = y.long()\n",
        "    tensordata = data_utils.TensorDataset(X, mask, y)\n",
        "    loader = data_utils.DataLoader(tensordata, batch_size=batch_size, shuffle=False)\n",
        "    return loader\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(index_padded, target_variable, \n",
        "                                                    test_size=0.1, random_state=42)\n",
        "\n",
        "train_masks, test_masks, _, _ = train_test_split(mask_variable, index_padded, \n",
        "                                                       test_size=0.1, random_state=42)\n",
        "\n",
        "trainloader = format_tensors(X_train, train_masks, y_train,BATCH_SIZE)\n",
        "testloader = format_tensors(X_test, test_masks, y_test, BATCH_SIZE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "menmobrqM-YY",
        "outputId": "d83806f8-65c6-4403-b674-b988822c4099"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased')\n",
        "model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMh3yB0WNZLl"
      },
      "source": [
        "def compute_accuracy(model, dataloader, device, m=1):\n",
        "    tqdm()\n",
        "    model.eval()\n",
        "    correct_preds, num_samples = 0,0\n",
        "    inc = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(dataloader)):\n",
        "            if i > 110: break\n",
        "            token_ids, masks, labels = tuple(t.to(device) for t in batch)\n",
        "            output = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n",
        "            yhat = output.logits\n",
        "            prediction = (torch.sigmoid(yhat[:,1]) > 0.5).long()\n",
        "            num_samples += labels.size(0)\n",
        "            correct_preds += (prediction==labels.long()).sum()\n",
        "            del token_ids, masks, labels #memory\n",
        "        torch.cuda.empty_cache() #memory\n",
        "        gc.collect() # memory\n",
        "        return correct_preds.float()/num_samples *100"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "1IlEbmAZNc0G",
        "outputId": "2f508e16-2b0e-4090-e2e5-21af5ad37896"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache() #memory\n",
        "gc.collect() #memory\n",
        "NUM_EPOCHS = 1\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "losses = []\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    iteration = 0\n",
        "    for i, batch in enumerate(trainloader):\n",
        "        if i > 120: break\n",
        "        iteration += 1\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n",
        "        loss = output.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += float(loss.item())\n",
        "        del token_ids, masks, labels #memory\n",
        "    \n",
        "        if not i%25:\n",
        "            print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                  f'Batch {i+1:03d}/{len(trainloader):03d} | '\n",
        "                  f'Average Loss in last {iteration} iteration(s): {(running_loss/iteration):.4f}')\n",
        "            running_loss = 0.0\n",
        "            iteration = 0\n",
        "        torch.cuda.empty_cache() #memory\n",
        "        gc.collect() #memory\n",
        "        losses.append(float(loss.item()))\n",
        "    # with torch.set_grad_enabled(False):\n",
        "    #     print(f'\\nTraining Accuracy: '\n",
        "    #           f'{compute_accuracy(model, trainloader, device):.2f}%')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001/001 | Batch 001/986 | Average Loss in last 1 iteration(s): 0.6010\n",
            "Epoch: 001/001 | Batch 026/986 | Average Loss in last 25 iteration(s): 0.5126\n",
            "Epoch: 001/001 | Batch 051/986 | Average Loss in last 25 iteration(s): 0.3493\n",
            "Epoch: 001/001 | Batch 076/986 | Average Loss in last 25 iteration(s): 0.2471\n",
            "Epoch: 001/001 | Batch 101/986 | Average Loss in last 25 iteration(s): 0.1436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            " 16%|█▌        | 159/986 [01:24<07:24,  1.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-274fbd1b9ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         print(f'\\nTraining Accuracy: '\n\u001b[0m\u001b[1;32m     36\u001b[0m               f'{compute_accuracy(model, trainloader, device):.2f}%')\n",
            "\u001b[0;32m<ipython-input-15-bff07cc9bc85>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-bff07cc9bc85>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB2XsBWfileN",
        "outputId": "3c7fe52a-a2f1-4cfe-b3f7-84b486fe9954"
      },
      "source": [
        "with torch.set_grad_enabled(False):\n",
        "    print(f'\\nTraining Accuracy: 'f'{compute_accuracy(model, trainloader, device):.2f}%')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/986 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 2/986 [00:00<05:04,  3.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 3/986 [00:01<06:11,  2.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 4/986 [00:01<06:58,  2.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 5/986 [00:02<07:31,  2.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 6/986 [00:02<07:55,  2.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 7/986 [00:03<08:12,  1.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 8/986 [00:03<08:22,  1.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 9/986 [00:04<08:30,  1.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 10/986 [00:04<08:38,  1.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 11/986 [00:05<08:40,  1.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 12/986 [00:06<08:42,  1.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|▏         | 13/986 [00:06<08:43,  1.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  1%|▏         | 14/986 [00:07<08:44,  1.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 15/986 [00:07<08:45,  1.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 16/986 [00:08<08:46,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 17/986 [00:08<08:45,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 18/986 [00:09<08:45,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 19/986 [00:09<08:44,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 20/986 [00:10<08:45,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 21/986 [00:10<08:45,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 22/986 [00:11<08:44,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 23/986 [00:12<08:46,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 24/986 [00:12<08:46,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 25/986 [00:13<08:46,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 26/986 [00:13<08:46,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 27/986 [00:14<08:44,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 28/986 [00:14<08:44,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 29/986 [00:15<08:44,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 30/986 [00:15<08:44,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 31/986 [00:16<08:44,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 32/986 [00:16<08:45,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 33/986 [00:17<08:45,  1.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 34/986 [00:18<08:43,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▎         | 35/986 [00:18<08:44,  1.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▎         | 36/986 [00:19<08:44,  1.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 37/986 [00:19<08:41,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 38/986 [00:20<08:41,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 39/986 [00:20<08:39,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 40/986 [00:21<08:38,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 41/986 [00:21<08:37,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 42/986 [00:22<08:36,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 43/986 [00:23<08:36,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 44/986 [00:23<08:34,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 45/986 [00:24<08:33,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 46/986 [00:24<08:33,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 47/986 [00:25<08:33,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 48/986 [00:25<08:32,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 49/986 [00:26<08:32,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 50/986 [00:26<08:32,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 51/986 [00:27<08:31,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 52/986 [00:27<08:31,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 53/986 [00:28<08:31,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 54/986 [00:29<08:30,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 55/986 [00:29<08:30,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 56/986 [00:30<08:30,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 57/986 [00:30<08:29,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 58/986 [00:31<08:27,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 59/986 [00:31<08:25,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 60/986 [00:32<08:25,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 61/986 [00:32<08:24,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▋         | 62/986 [00:33<08:24,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▋         | 63/986 [00:33<08:23,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▋         | 64/986 [00:34<08:23,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 65/986 [00:35<08:24,  1.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 66/986 [00:35<08:22,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 67/986 [00:36<08:22,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 68/986 [00:36<08:20,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 69/986 [00:37<08:21,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 70/986 [00:37<08:19,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 71/986 [00:38<08:20,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 72/986 [00:38<08:17,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 73/986 [00:39<08:17,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 74/986 [00:39<08:17,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 75/986 [00:40<08:17,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 76/986 [00:41<08:16,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 77/986 [00:41<08:15,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 78/986 [00:42<08:14,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 79/986 [00:42<08:16,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 80/986 [00:43<08:15,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 81/986 [00:43<08:13,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 82/986 [00:44<08:12,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 83/986 [00:44<08:13,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▊         | 84/986 [00:45<08:12,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▊         | 85/986 [00:45<08:11,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▊         | 86/986 [00:46<08:10,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 87/986 [00:47<08:08,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 88/986 [00:47<08:08,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 89/986 [00:48<08:07,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 90/986 [00:48<08:07,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 91/986 [00:49<08:05,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 92/986 [00:49<08:04,  1.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 93/986 [00:50<08:04,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 94/986 [00:50<08:04,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 95/986 [00:51<08:05,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 96/986 [00:51<08:05,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 97/986 [00:52<08:04,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 98/986 [00:53<08:03,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 99/986 [00:53<08:05,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 100/986 [00:54<08:03,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 101/986 [00:54<08:03,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 102/986 [00:55<08:02,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 103/986 [00:55<08:00,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 104/986 [00:56<08:01,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 105/986 [00:56<08:00,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 106/986 [00:57<07:59,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 107/986 [00:57<07:59,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 108/986 [00:58<07:58,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 109/986 [00:59<07:57,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 110/986 [00:59<07:58,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█▏        | 111/986 [01:00<07:55,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Accuracy: 94.92%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAlD1PpOdJ3u",
        "outputId": "4f7707d7-d211-482f-ee54-f67c5e412607"
      },
      "source": [
        "with torch.set_grad_enabled(False):\n",
        "  print(f'\\n\\nTest Accuracy:'\n",
        "  f'{compute_accuracy(model, testloader, device, 2):.2f}%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "  0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 2/110 [00:00<00:32,  3.31it/s]\u001b[A\n",
            "  3%|▎         | 3/110 [00:01<00:39,  2.70it/s]\u001b[A\n",
            "  4%|▎         | 4/110 [00:01<00:44,  2.38it/s]\u001b[A\n",
            "  5%|▍         | 5/110 [00:02<00:47,  2.20it/s]\u001b[A\n",
            "  5%|▌         | 6/110 [00:02<00:49,  2.09it/s]\u001b[A\n",
            "  6%|▋         | 7/110 [00:03<00:51,  2.01it/s]\u001b[A\n",
            "  7%|▋         | 8/110 [00:03<00:51,  1.97it/s]\u001b[A\n",
            "  8%|▊         | 9/110 [00:04<00:52,  1.94it/s]\u001b[A\n",
            "  9%|▉         | 10/110 [00:04<00:52,  1.92it/s]\u001b[A\n",
            " 10%|█         | 11/110 [00:05<00:52,  1.90it/s]\u001b[A\n",
            " 11%|█         | 12/110 [00:05<00:51,  1.89it/s]\u001b[A\n",
            " 12%|█▏        | 13/110 [00:06<00:51,  1.88it/s]\u001b[A\n",
            " 13%|█▎        | 14/110 [00:07<00:51,  1.87it/s]\u001b[A\n",
            " 14%|█▎        | 15/110 [00:07<00:50,  1.88it/s]\u001b[A\n",
            " 15%|█▍        | 16/110 [00:08<00:50,  1.87it/s]\u001b[A\n",
            " 15%|█▌        | 17/110 [00:08<00:49,  1.87it/s]\u001b[A\n",
            " 16%|█▋        | 18/110 [00:09<00:49,  1.87it/s]\u001b[A\n",
            " 17%|█▋        | 19/110 [00:09<00:48,  1.87it/s]\u001b[A\n",
            " 18%|█▊        | 20/110 [00:10<00:48,  1.87it/s]\u001b[A\n",
            " 19%|█▉        | 21/110 [00:10<00:47,  1.86it/s]\u001b[A\n",
            " 20%|██        | 22/110 [00:11<00:47,  1.86it/s]\u001b[A\n",
            " 21%|██        | 23/110 [00:11<00:46,  1.87it/s]\u001b[A\n",
            " 22%|██▏       | 24/110 [00:12<00:46,  1.86it/s]\u001b[A\n",
            " 23%|██▎       | 25/110 [00:12<00:45,  1.86it/s]\u001b[A\n",
            " 24%|██▎       | 26/110 [00:13<00:45,  1.86it/s]\u001b[A\n",
            " 25%|██▍       | 27/110 [00:14<00:44,  1.86it/s]\u001b[A\n",
            " 25%|██▌       | 28/110 [00:14<00:44,  1.86it/s]\u001b[A\n",
            " 26%|██▋       | 29/110 [00:15<00:43,  1.86it/s]\u001b[A\n",
            " 27%|██▋       | 30/110 [00:15<00:43,  1.86it/s]\u001b[A\n",
            " 28%|██▊       | 31/110 [00:16<00:42,  1.86it/s]\u001b[A\n",
            " 29%|██▉       | 32/110 [00:16<00:42,  1.85it/s]\u001b[A\n",
            " 30%|███       | 33/110 [00:17<00:41,  1.85it/s]\u001b[A\n",
            " 31%|███       | 34/110 [00:17<00:40,  1.86it/s]\u001b[A\n",
            " 32%|███▏      | 35/110 [00:18<00:40,  1.86it/s]\u001b[A\n",
            " 33%|███▎      | 36/110 [00:18<00:39,  1.86it/s]\u001b[A\n",
            " 34%|███▎      | 37/110 [00:19<00:39,  1.85it/s]\u001b[A\n",
            " 35%|███▍      | 38/110 [00:19<00:38,  1.86it/s]\u001b[A\n",
            " 35%|███▌      | 39/110 [00:20<00:38,  1.85it/s]\u001b[A\n",
            " 36%|███▋      | 40/110 [00:21<00:37,  1.85it/s]\u001b[A\n",
            " 37%|███▋      | 41/110 [00:21<00:37,  1.86it/s]\u001b[A\n",
            " 38%|███▊      | 42/110 [00:22<00:36,  1.85it/s]\u001b[A\n",
            " 39%|███▉      | 43/110 [00:22<00:36,  1.85it/s]\u001b[A\n",
            " 40%|████      | 44/110 [00:23<00:35,  1.85it/s]\u001b[A\n",
            " 41%|████      | 45/110 [00:23<00:35,  1.86it/s]\u001b[A\n",
            " 42%|████▏     | 46/110 [00:24<00:34,  1.86it/s]\u001b[A\n",
            " 43%|████▎     | 47/110 [00:24<00:33,  1.86it/s]\u001b[A\n",
            " 44%|████▎     | 48/110 [00:25<00:33,  1.85it/s]\u001b[A\n",
            " 45%|████▍     | 49/110 [00:25<00:32,  1.85it/s]\u001b[A\n",
            " 45%|████▌     | 50/110 [00:26<00:32,  1.85it/s]\u001b[A\n",
            " 46%|████▋     | 51/110 [00:26<00:31,  1.85it/s]\u001b[A\n",
            " 47%|████▋     | 52/110 [00:27<00:31,  1.85it/s]\u001b[A\n",
            " 48%|████▊     | 53/110 [00:28<00:30,  1.85it/s]\u001b[A\n",
            " 49%|████▉     | 54/110 [00:28<00:30,  1.85it/s]\u001b[A\n",
            " 50%|█████     | 55/110 [00:29<00:29,  1.86it/s]\u001b[A\n",
            " 51%|█████     | 56/110 [00:29<00:29,  1.85it/s]\u001b[A\n",
            " 52%|█████▏    | 57/110 [00:30<00:28,  1.85it/s]\u001b[A\n",
            " 53%|█████▎    | 58/110 [00:30<00:28,  1.85it/s]\u001b[A\n",
            " 54%|█████▎    | 59/110 [00:31<00:27,  1.86it/s]\u001b[A\n",
            " 55%|█████▍    | 60/110 [00:31<00:26,  1.86it/s]\u001b[A\n",
            " 55%|█████▌    | 61/110 [00:32<00:26,  1.86it/s]\u001b[A\n",
            " 56%|█████▋    | 62/110 [00:32<00:25,  1.85it/s]\u001b[A\n",
            " 57%|█████▋    | 63/110 [00:33<00:25,  1.85it/s]\u001b[A\n",
            " 58%|█████▊    | 64/110 [00:33<00:24,  1.85it/s]\u001b[A\n",
            " 59%|█████▉    | 65/110 [00:34<00:24,  1.85it/s]\u001b[A\n",
            " 60%|██████    | 66/110 [00:35<00:23,  1.86it/s]\u001b[A\n",
            " 61%|██████    | 67/110 [00:35<00:23,  1.86it/s]\u001b[A\n",
            " 62%|██████▏   | 68/110 [00:36<00:22,  1.85it/s]\u001b[A\n",
            " 63%|██████▎   | 69/110 [00:36<00:22,  1.86it/s]\u001b[A\n",
            " 64%|██████▎   | 70/110 [00:37<00:21,  1.86it/s]\u001b[A\n",
            " 65%|██████▍   | 71/110 [00:37<00:20,  1.86it/s]\u001b[A\n",
            " 65%|██████▌   | 72/110 [00:38<00:20,  1.86it/s]\u001b[A\n",
            " 66%|██████▋   | 73/110 [00:38<00:19,  1.86it/s]\u001b[A\n",
            " 67%|██████▋   | 74/110 [00:39<00:19,  1.85it/s]\u001b[A\n",
            " 68%|██████▊   | 75/110 [00:39<00:18,  1.85it/s]\u001b[A\n",
            " 69%|██████▉   | 76/110 [00:40<00:18,  1.86it/s]\u001b[A\n",
            " 70%|███████   | 77/110 [00:40<00:17,  1.86it/s]\u001b[A\n",
            " 71%|███████   | 78/110 [00:41<00:17,  1.86it/s]\u001b[A\n",
            " 72%|███████▏  | 79/110 [00:42<00:16,  1.86it/s]\u001b[A\n",
            " 73%|███████▎  | 80/110 [00:42<00:16,  1.85it/s]\u001b[A\n",
            " 74%|███████▎  | 81/110 [00:43<00:15,  1.85it/s]\u001b[A\n",
            " 75%|███████▍  | 82/110 [00:43<00:15,  1.85it/s]\u001b[A\n",
            " 75%|███████▌  | 83/110 [00:44<00:14,  1.85it/s]\u001b[A\n",
            " 76%|███████▋  | 84/110 [00:44<00:14,  1.85it/s]\u001b[A\n",
            " 77%|███████▋  | 85/110 [00:45<00:13,  1.85it/s]\u001b[A\n",
            " 78%|███████▊  | 86/110 [00:45<00:12,  1.85it/s]\u001b[A\n",
            " 79%|███████▉  | 87/110 [00:46<00:12,  1.85it/s]\u001b[A\n",
            " 80%|████████  | 88/110 [00:46<00:11,  1.84it/s]\u001b[A\n",
            " 81%|████████  | 89/110 [00:47<00:11,  1.84it/s]\u001b[A\n",
            " 82%|████████▏ | 90/110 [00:47<00:10,  1.85it/s]\u001b[A\n",
            " 83%|████████▎ | 91/110 [00:48<00:10,  1.85it/s]\u001b[A\n",
            " 84%|████████▎ | 92/110 [00:49<00:09,  1.86it/s]\u001b[A\n",
            " 85%|████████▍ | 93/110 [00:49<00:09,  1.86it/s]\u001b[A\n",
            " 85%|████████▌ | 94/110 [00:50<00:08,  1.85it/s]\u001b[A\n",
            " 86%|████████▋ | 95/110 [00:50<00:08,  1.86it/s]\u001b[A\n",
            " 87%|████████▋ | 96/110 [00:51<00:07,  1.85it/s]\u001b[A\n",
            " 88%|████████▊ | 97/110 [00:51<00:07,  1.86it/s]\u001b[A\n",
            " 89%|████████▉ | 98/110 [00:52<00:06,  1.85it/s]\u001b[A\n",
            " 90%|█████████ | 99/110 [00:52<00:05,  1.85it/s]\u001b[A\n",
            " 91%|█████████ | 100/110 [00:53<00:05,  1.86it/s]\u001b[A\n",
            " 92%|█████████▏| 101/110 [00:53<00:04,  1.86it/s]\u001b[A\n",
            " 93%|█████████▎| 102/110 [00:54<00:04,  1.86it/s]\u001b[A\n",
            " 94%|█████████▎| 103/110 [00:55<00:03,  1.84it/s]\u001b[A\n",
            " 95%|█████████▍| 104/110 [00:55<00:03,  1.86it/s]\u001b[A\n",
            " 95%|█████████▌| 105/110 [00:56<00:02,  1.86it/s]\u001b[A\n",
            " 96%|█████████▋| 106/110 [00:56<00:02,  1.86it/s]\u001b[A\n",
            " 97%|█████████▋| 107/110 [00:57<00:01,  1.86it/s]\u001b[A\n",
            " 98%|█████████▊| 108/110 [00:57<00:01,  1.85it/s]\u001b[A\n",
            " 99%|█████████▉| 109/110 [00:58<00:00,  1.86it/s]\u001b[A\n",
            "100%|██████████| 110/110 [00:58<00:00,  1.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Test Accuracy:89.83%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}